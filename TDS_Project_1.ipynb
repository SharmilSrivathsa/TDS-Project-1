{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# GitHub Personal Access Token for authenticated requests\n",
        "GITHUB_TOKEN = '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'\n",
        "headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
        "\n",
        "# Fetch users located in Basel with more than 10 followers\n",
        "def get_users(location='Basel', min_followers=10):\n",
        "    url = f\"https://api.github.com/search/users?q=location:{location}+followers:>{min_followers}&per_page=100\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    users = response.json().get('items', [])\n",
        "    return [user['login'] for user in users]\n",
        "\n",
        "# Get detailed information for each user\n",
        "def get_user_details(username):\n",
        "    url = f\"https://api.github.com/users/{username}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    # Clean and format company names as specified\n",
        "    company = data.get('company', '')\n",
        "    if company:\n",
        "        company = company.strip().lstrip('@').upper()  # Clean up company name\n",
        "\n",
        "    return {\n",
        "        'login': data.get('login', ''),\n",
        "        'name': data.get('name', ''),\n",
        "        'company': company,\n",
        "        'location': data.get('location', ''),\n",
        "        'email': data.get('email', ''),\n",
        "        'hireable': data.get('hireable', ''),\n",
        "        'bio': data.get('bio', ''),\n",
        "        'public_repos': data.get('public_repos', 0),\n",
        "        'followers': data.get('followers', 0),\n",
        "        'following': data.get('following', 0),\n",
        "        'created_at': data.get('created_at', '')\n",
        "    }\n",
        "\n",
        "# Get up to 500 repositories for each user\n",
        "def get_repositories(username):\n",
        "    repos = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        url = f\"https://api.github.com/users/{username}/repos?per_page=100&page={page}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if not data:\n",
        "            break\n",
        "        for repo in data:\n",
        "            # Handle cases where license information may be missing\n",
        "            license_info = repo.get('license')\n",
        "            license_name = license_info['key'] if license_info else ''  # Default to empty string if no license\n",
        "\n",
        "            repos.append({\n",
        "                'login': username,  # Add the user's login as a reference\n",
        "                'full_name': repo.get('full_name', ''),\n",
        "                'created_at': repo.get('created_at', ''),\n",
        "                'stargazers_count': repo.get('stargazers_count', 0),\n",
        "                'watchers_count': repo.get('watchers_count', 0),\n",
        "                'language': repo.get('language', ''),\n",
        "                'has_projects': repo.get('has_projects', False),\n",
        "                'has_wiki': repo.get('has_wiki', False),\n",
        "                'license_name': license_name\n",
        "            })\n",
        "        if len(data) < 100 or len(repos) >= 500:\n",
        "            break\n",
        "        page += 1\n",
        "        time.sleep(1)  # Avoid hitting rate limits\n",
        "    return repos[:500]\n",
        "\n",
        "# Main function to orchestrate data fetching and saving\n",
        "def main():\n",
        "    # Fetch users\n",
        "    usernames = get_users()\n",
        "    users_data = []\n",
        "    repos_data = []\n",
        "\n",
        "    # Fetch details for each user and their repositories\n",
        "    for username in usernames:\n",
        "        user_info = get_user_details(username)\n",
        "        users_data.append(user_info)\n",
        "\n",
        "        repos = get_repositories(username)\n",
        "        repos_data.extend(repos)\n",
        "\n",
        "    # Save to CSV files\n",
        "    users_df = pd.DataFrame(users_data)\n",
        "    repos_df = pd.DataFrame(repos_data)\n",
        "    users_df.to_csv('users.csv', index=False)\n",
        "    repos_df.to_csv('repositories.csv', index=False)\n",
        "    print(\"Data successfully scraped and saved to users.csv and repositories.csv.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6YyYWjfhn1k",
        "outputId": "eff8e908-0d21-4b1c-b979-8f877cd7beaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully scraped and saved to users.csv and repositories.csv.\n"
          ]
        }
      ]
    }
  ]
}